　　阅读了知乎文章[《知识蒸馏是什么？一份入门随笔》](https://zhuanlan.zhihu.com/p/90049906)的1、2节。对知识蒸馏的思想有了一个初步的理解。通过采用预先训练好的复杂模型（Teacher model）的输出作为监督信号去训练另外一个简单的网络。这个简单的网络称之为student model。

　　第一次接触知识蒸馏，是**想用BERT指导对话生成任务**。

　　阅读了论文[Distilling Knowledge Learned in BERT for Text Generation](https://www.aclweb.org/anthology/2020.acl-main.705.pdf)，及其代码[ChenRocks/Distill-BERT-Textgen](https://github.com/ChenRocks/Distill-BERT-Textgen)。

　　该论文的思路：The finetuned BERT (teacher) is exploited as extra supervision to improve conventional Seq2Seq models (student) for better text generation performance. 用BERT指导seq2seq进行文本生成任务。（代码中是机器翻译任务）

　　代码是用docker运行的，虽然看不懂docker的命令，但是可以大概了解文件运行的顺序。opennmt文件夹是作者的另一个库，[Distill-BERT-Textgen-ONMT](https://github.com/ChenRocks/Distill-BERT-Textgen-ONMT)。是在机器翻译的代码（OpenNMT-py）上进行了修改。This fork of OpenNMT-py is modified to suppoprt training of this [ACL 2020 paper](https://arxiv.org/abs/1911.03829)（就是上面那篇）, which can be used through this [repo](https://github.com/ChenRocks/Distill-BERT-Textgen)（就是Distill-BERT-Textgen）. The differences can be viewed by comparing the [commits](https://github.com/ChenRocks/Distill-BERT-Textgen-ONMT/compare/onmt...master).

　　看了一下数据集的获取，挺麻烦的，看不太懂。有docker的命令，也有shell的语法，劝退了。

　　我猜代码的主要部分可能是Teacher model的finetune？这部分应该可以看懂。但是在commits里面看了一下对OpenNMT-py的修改，这部分就非常有难度了，又劝退了。所以我大概是放弃知识蒸馏了。

<p align="right">2021.10.12</p>

